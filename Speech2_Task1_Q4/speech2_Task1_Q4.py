# -*- coding: utf-8 -*-
"""SPA_assign_2_1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14XWkuJp22vegKRma6mHgVPEpxpP_ndOs
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install wandb
# !pip install speechbrain

from google.colab import drive
drive.mount('/content/drive')

! wandb login c07f9b9363b1d2736cf24c01a4747e245909f2cc

import os
import numpy as np
import tarfile
from zipfile import ZipFile
import sklearn.metrics as metrics

import torch
import torchaudio
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchaudio as ta
import wandb

from transformers import Wav2Vec2FeatureExtractor, WavLMForXVector, UniSpeechSatForXVector, HubertForSequenceClassification
from speechbrain.inference.speaker import EncoderClassifier

with tarfile.open("/content/drive/MyDrive/FinalSpeech/VoxCeleb1_subset.tar-tronâ€™s MacBook Pro.gz") as tar:
  tar.extractall("/content/")

!mv -v /content/VoxCeleb1_subset/test/* /content/VoxCeleb1_subset/dev/

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
cosine_similarity = nn.CosineSimilarity(dim=-1)

class CustomDataset(torch.utils.data.Dataset):
  def __init__(self, root_dir, txt_file, max_frames=32000):
    self.root_dir = root_dir
    self.txt_file = txt_file
    self.max_frames = max_frames
    self.data = self.read_file()

  def read_file(self):
    data = []
    with open(self.txt_file, 'r') as fil:
      for line in fil:
        label, first, second = line.strip().split()

        second_path = os.path.join(self.root_dir, second)
        first_path = os.path.join(self.root_dir, first)

        if os.path.exists(first_path) :
          if os.path.exists(second_path):
              data.append((label, first, second))

    return data

  def __len__(self):
    return len(self.data)

  def process_sample(self, path):
    filename, _ = os.path.splitext(path)
    path = filename + ".wav"
    file_path = os.path.join(self.root_dir, path)

    wav, sample_rate = ta.load(file_path)
    num_frames = wav.shape[1]

    if num_frames >= self.max_frames:
      wav = wav[:, :self.max_frames]
    else:
      pad_size = self.max_frames - num_frames
      wav = F.pad(wav, (0, pad_size), value=0)

    return wav, sample_rate

  def __getitem__(self, idx):
    label, first_path, second_path = self.data[idx]
    first_tensor, first_sample_rate = self.process_sample(first_path)
    second_tensor, second_sample_rate = self.process_sample(second_path)

    first_tensor = first_tensor.squeeze(0)
    second_tensor = second_tensor.squeeze(0)
    label = torch.tensor(np.array(int(label)))

    return first_tensor, second_tensor, label

def compute_eer(labels, preds):
    fpr, tpr, thresholds = metrics.roc_curve(labels, preds, pos_label=1)
    eer = np.min(np.abs(fpr - (1 - tpr)))
    return eer

def evaluate(model, test_loader, extractor, cos_sim):
    model.eval()
    total_eer = 0

    for batch_idx, (wav1, wav2, label) in enumerate(test_loader):
      wav1 = wav1.to(device)
      wav2 = wav2.to(device)

      with torch.inference_mode():
          audio1 = extractor(wav1.squeeze(0), return_tensors="pt", sampling_rate=16000).input_values.squeeze(0).to(device)
          audio2 = extractor(wav2.squeeze(0), return_tensors="pt", sampling_rate=16000).input_values.squeeze(0).to(device)

          embeddings1 = F.normalize(model(input_values=audio1).embeddings,dim=-1).cpu()
          embeddings2 = F.normalize(model(input_values=audio2).embeddings,dim=-1).cpu()

          similarity = torch.sigmoid(cos_sim(embeddings1, embeddings2))
          eer = compute_eer(label, similarity)
          total_eer += eer

          if batch_idx % 50 == 0:
            print(f"{batch_idx+1}/{len(test_loader)} EER: {eer}")

    total_eer = total_eer / len(test_loader)
    return total_eer

def evaluate_ecapa(classifier, test_dataloader, cos_sim):
    total_eer = 0
    for batch_idx, (wav1, wav2, label) in enumerate(test_dataloader):
      wav1 = wav1.to(device)
      wav2 = wav2.to(device)

      with torch.inference_mode():
          embeddings1 = (F.normalize(classifier.encode_batch(wav1),dim=-1)).cpu()
          embeddings2 = (F.normalize(classifier.encode_batch(wav2),dim=-1)).cpu()

          similarity = torch.sigmoid(cos_sim(embeddings1, embeddings2))
          eer = compute_eer(label, similarity)
          total_eer += eer

          if batch_idx % 50 == 0:
            print(f"{batch_idx+1}/{len(test_dataloader)} EER: {eer}")

    total_eer = total_eer / len(test_dataloader)

    return total_eer

"""# VoxCeleb1-H"""

# wandb.init(project="Speech Assignment Task 1", name="Voxceleb1-H")

# txt_file = "/content/VoxCeleb1_subset/list_test_hard.txt"
# wav_dir = "/content/VoxCeleb1_subset/dev"

# vox_test_dataset = CustomDataset(wav_dir, txt_file, 32000)
# vox_test_loader = DataLoader(vox_test_dataset, batch_size=64, shuffle=True)

"""# 1. Ecapa TDNN"""

# classifier = EncoderClassifier.from_hparams(source="speechbrain/spkrec-ecapa-voxceleb", run_opts={"device":"cuda"} )
# vox_ecapa_eer = round(evaluate_ecapa(classifier,vox_test_loader,cosine_similarity),4)
# wandb.log({'ECAPA VOX EER': vox_ecapa_eer})
# print(f"Average EER(%): {vox_ecapa_eer * 100}%")

"""# 2. Unispeech-sat-base"""

# model = UniSpeechSatForXVector.from_pretrained('microsoft/unispeech-sat-base-sv').to(device)
# extractor = Wav2Vec2FeatureExtractor.from_pretrained('microsoft/unispeech-sat-base-sv')
# vox_uni_eer = round(evaluate(model,vox_test_loader,extractor,cosine_similarity),4)
# wandb.log({f'Unispeech VOX EER': vox_uni_eer})
# print(f"Average EER(%): {vox_uni_eer * 100}%")

"""# 3. Wavlm-base-plus"""

# model = WavLMForXVector.from_pretrained('microsoft/wavlm-base-plus-sv').to(device)
# extractor = Wav2Vec2FeatureExtractor.from_pretrained('microsoft/wavlm-base-plus-sv')
# vox_wavlm_eer = round(evaluate(model,vox_test_loader,extractor,cosine_similarity),4)
# wandb.log({f'Wavlm VOX EER': vox_wavlm_eer})
# print(f"Average EER(%): {vox_wavlm_eer * 100}")

# wandb.finish()

"""# Kathbath Dataset"""

# wandb.init(project="Speech Assignment Task 1", name="Kathbath Dataset")

# os.makedirs("/content/kb_test_hi/", exist_ok=True)
# with ZipFile("/content/drive/MyDrive/FinalSpeech/kb_test_hi.zip", "r") as kb:
#   kb.extractall("/content/kb_test_hi/")

# # drive_folder = "/content/drive/MyDrive/"
# # kb_val_dir = "/content/kb_val_hi/wav"
# kb_test_dir = "/content/kb_test_hi"
# kb_test_pairs = "/content/drive/MyDrive/FinalSpeech/kb_test_pairs.txt"
# kb_val_pairs = "/content/drive/MyDrive/FinalSpeech/kb_valid_pairs.txt"

# kb_test_dataset = CustomDataset(kb_test_dir, kb_test_pairs, 32000)
# kb_test_loader = DataLoader(kb_test_dataset, batch_size=64, shuffle=True)

"""# 1. Ecapa TDNN"""

# classifier = EncoderClassifier.from_hparams(source="speechbrain/spkrec-ecapa-voxceleb", run_opts={"device":"cuda"} )
# kb_ecapa_eer = round(evaluate_ecapa(classifier,kb_test_loader,cosine_similarity),4)
# wandb.log({'ECAPA KB EER': kb_ecapa_eer})
# print(f"Average EER(%): {kb_ecapa_eer * 100}%")

"""# 2. Unispeech-sat-base"""

# model = UniSpeechSatForXVector.from_pretrained('microsoft/unispeech-sat-base-sv').to(device)
# extractor = Wav2Vec2FeatureExtractor.from_pretrained('microsoft/unispeech-sat-base-sv')
# kb_uni_eer = round(evaluate(model,kb_test_loader,extractor,cosine_similarity),4)
# wandb.log({f'Unispeech KB EER': kb_uni_eer})
# print(f"Average EER(%): {kb_uni_eer * 100}%")

"""# 3. Wavlm-base-plus"""

# model = WavLMForXVector.from_pretrained('microsoft/wavlm-base-plus-sv').to(device)
# extractor = Wav2Vec2FeatureExtractor.from_pretrained('microsoft/wavlm-base-plus-sv')
# kb_wavlm_eer = round(evaluate(model,kb_test_loader,extractor,cosine_similarity),4)
# wandb.log({f'Wavlm KB EER': kb_wavlm_eer})
# print(f"Average EER(%): {kb_wavlm_eer * 100}")

# wandb.finish()

"""# Fine Tuning"""

!git clone https://github.com/speechbrain/speechbrain.git

from zipfile import ZipFile

# os.makedirs("/content/kb_test_hi/", exist_ok=True)

with ZipFile("/content/drive/MyDrive/FinalSpeech/valid.zip", "r") as kb:
  kb.extractall("/content/")

!pip install -r ../content/speechbrain/requirements.txt
!pip install -e .

!python /content/speechbrain/recipes/VoxCeleb/SpeakerRec/train_speaker_embeddings.py /content/speechbrain/recipes/VoxCeleb/SpeakerRec/hparams/train_ecapa_tdnn.yaml --data_folder=/content/valid

